{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bilsulfite Pipeline\n",
    "\n",
    "![bisulfite_pipeline](https://ars.els-cdn.com/content/image/1-s2.0-S0168165617315936-gr1_lrg.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.1 QC Step\n",
    "\n",
    "* 2.1 Install following softwares\n",
    "\n",
    "FastQC : FastQC aims to provide a simple way to do some quality control checks on raw sequence data coming from high throughput sequencing pipelines. It provides a modular set of analyses which you can use to give a quick impression of whether your data has any problems of which you should be aware before doing any further analysis.\n",
    "\n",
    "MultiQC : A modular tool to aggregate results from bioinformatics analyses across many samples into a single report."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Pre Alignment QC Report can be viewed at:\n",
    "https://htmlpreview.github.io/?https://github.com/cjgunase/CoRSIV/blob/master/files/multiqc_report.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.2 Split reads in each FASTQ file in to 1M read files for parallel processing\n",
    "\n",
    "\n",
    "each sample is about 40GB file, so it will be very difficult to process unless with large amount of memory in each compute node.so I split each file in to small 10M read files and processed parellel for the efficiency.\n",
    "\n",
    "```bash\n",
    "#!/bin/sh\n",
    "\n",
    "#PBS -N fastqc\n",
    "#PBS -l nodes=2:ppn=4\n",
    "#PBS -l walltime=6:00:00\n",
    "#PBS -q dque\n",
    "#PBS -e . -o .\n",
    "\n",
    "# This job's temporary working directory. You may also work in any of your\n",
    "# home directories\n",
    "echo Working directory is $PBS_O_WORKDIR\n",
    "echo Running on host `hostname`\n",
    "echo Time is `date`\n",
    "echo Directory is `pwd`\n",
    "echo This job runs on the following processors:\n",
    "echo \"PBS_NODEFILE=\" $PBS_NODEFILE\n",
    "\n",
    "cd /store1_e/waterland/gunasekara/GTeX_File_System/raw_fq/GTEX-13OW7-0826-SM-D5A5L/\n",
    "\n",
    "#Args: Input Fastq files\n",
    "\n",
    "IN_FILE=$1\n",
    "echo $IN_FILE\n",
    "\n",
    "mkdir -p split\n",
    "\n",
    "e=$(echo $IN_FILE|cut -f1,2 -d\".\")\n",
    "echo $e\n",
    "\n",
    "#zcat $IN_FILE | split --verbose -l 40000000 -d -a 4 --filter=\"pigz -p 2 -c > split/\\$FILE.gz\" - split.10m.$e\n",
    "zcat $IN_FILE | split --verbose -l 40000000 -d -a 4 --filter=\"pigz -p 2 -c > $FILE.gz\" - split.10m.$e\n",
    "```\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1.3 TrimGalore \n",
    "```shell\n",
    "#!/bin/sh\n",
    "\n",
    "#PBS -N bismark\n",
    "#PBS -l nodes=1:ppn=8\n",
    "#PBS -l walltime=4:00:00\n",
    "#PBS -q dque\n",
    "#PBS -o /store1_e/waterland/gunasekara/\n",
    "#PBS -e /store1_e/waterland/gunasekara/\n",
    "\n",
    "DIR_NAME=$(dirname $1)\n",
    "\n",
    "mkdir $DIR_NAME/trimmed\n",
    "\n",
    "OUTPUT_DIR=$DIR_NAME/trimmed\n",
    "\n",
    "#trim_galore --paired -q 20 --length 50 -o $OUTPUT_DIR $1 $2\n",
    "trim_galore --paired -o $OUTPUT_DIR $1 $2\n",
    "```\n",
    "each file is submitted process in a loop using correct pairing\n",
    "\n",
    "```python\n",
    "import glob\n",
    "import os\n",
    "\n",
    "read1 = sorted(glob.glob(\"./split.10m.GTEX-S7SE-2526-SM-D5A5E_1*\"))\n",
    "read2 = sorted(glob.glob(\"./split.10m.GTEX-S7SE-2526-SM-D5A5E_2*\"))\n",
    "\n",
    "for fqFile in list(range(31,70)):\n",
    "    cmd = \"sbatch /home/gunasekara/scripts/TrimG.sh \" + read1[fqFile] + \" \" + read2[fqFile]\n",
    "    print(cmd)\n",
    "    os.system(cmd)\n",
    "\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2 Align to hg38 [Reference Genome](https://en.wikipedia.org/wiki/Reference_genome)\n",
    "\n",
    "### 2.1 Download and index the hg38 reference genome\n",
    "\n",
    "\n",
    "### 2.2 align reads\n",
    "\n",
    "Description of the parameters used:\n",
    "\n",
    "-q : The query input files (specified as\n",
    "\n",
    "--bowtie2 : Default: ON. Uses Bowtie 2 instead of Bowtie 1. Bismark limits Bowtie 2 to only perform end-to-end alignments, i.e. searches for alignments involving all read characters (also called untrimmed or unclipped alignments). Bismark assumes that raw sequence data is adapter and/or quality trimmed where appropriate. Both small (.bt2) and large (.bt2l) Bowtie 2 indexes are supported.\n",
    "\n",
    "-p : This options if to paralleliztion of bowtie2\n",
    "\n",
    "[genome folder] in this example, the pathe to the genome folder used by Jack is given.\n",
    "\n",
    "-B : The base name of the .BAM file generated from the Bismark.\n",
    "\n",
    "--multicore : this option works when -B is not specified. Use this to set the programm to run in parallel.\n",
    "-1: read 1 fastq file\n",
    "-2: read 2 fastq file\n",
    "\n",
    "** Note that, if you are planning to use bisSNP later, add the --rg_tag to bismark. The the read groups will be tagged with some information about the sequecing for later use.\n",
    "\n",
    "\n",
    "```shell\n",
    "\n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --time=2-00:00 -n24 -p bynode\n",
    "\n",
    "# Arg order: First_read.fq Second_read.fq Read_Group_string\n",
    "\n",
    "starttime=$(date +\"%s\")\n",
    "echo $(date -u -d @${starttime})\n",
    "\n",
    "\n",
    "DIR_NAME=$(dirname $1)\n",
    "\n",
    "mkdir \"$DIR_NAME\"/aligned\n",
    "\n",
    "OUT_DIR=$DIR_NAME/aligned\n",
    "\n",
    "echo \"Output directory: $OUT_DIR\"\n",
    "\n",
    "bismark --multicore 6 -q -o $OUT_DIR --genome /home/scott/genomes/human/gencode -1 $1 -2 $2\n",
    "\n",
    "endtime=$(date +\"%s\")\n",
    "echo $(date -u -d @${endtime})\n",
    "echo \"Time elapsed\" $(date -u -d @$(($endtime-$starttime)) +\"%T\")\n",
    "\n",
    "```\n",
    "\n",
    "### 2.3 split each bam file in to chromosomes\n",
    "\n",
    "```python\n",
    "####################################\n",
    "# Because this is executing 24 jobs simultaneously it is\n",
    "# best to execute this as a batch script on the bynode queue\n",
    "# as it should use all of the available cores.\n",
    "#\n",
    "# Usage: \"python3 split_by_chromosome.py [-h] [--chr] input.bam\"\n",
    "#   Input bam must be coordinate sorted and have a .bai index file\n",
    "# But consider starting this with an sbatch job script\n",
    "#\n",
    "# Samtools must be installed and present on the PATH\n",
    "#\n",
    "####################################\n",
    "\n",
    "import subprocess\n",
    "import time\n",
    "import sys\n",
    "import argparse\n",
    "\n",
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument(\"input_bam\", help=\"Input bam file to split, file must have index present\")\n",
    "parser.add_argument(\"--chr\", help=\"Set if input bam has chromosome IDs beginning with 'chr'\", action=\"store_true\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "if args.chr:\n",
    "    chromosomes = [\"chr1\", \"chr2\", \"chr3\", \"chr4\", \"chr5\", \"chr6\", \"chr7\", \"chr8\", \"chr9\", \"chr10\",\n",
    "               \"chr11\", \"chr12\", \"chr13\", \"chr14\", \"chr15\", \"chr16\", \"chr17\", \"chr18\", \"chr19\",\n",
    "               \"chr20\", \"chr21\", \"chr22\", \"chrX\", \"chrY\"]\n",
    "else:\n",
    "    chromosomes = [\"1\", \"2\", \"3\", \"4\", \"5\", \"6\", \"7\", \"8\", \"9\", \"10\",\n",
    "                   \"11\", \"12\", \"13\", \"14\", \"15\", \"16\", \"17\", \"18\", \"19\",\n",
    "                   \"20\", \"21\", \"22\", \"X\", \"Y\"]\n",
    "\n",
    "\n",
    "# Get the input file from the command line\n",
    "input_bam = args.input_bam\n",
    "file_name = input_bam.split('.')[0]\n",
    "print(\"Input file is %s\" % input_bam)\n",
    "sys.stdout.flush()  # This is used to force a write to stdout on slurm\n",
    "\n",
    "# create list to hold all spawned job references\n",
    "procs = []\n",
    "\n",
    "# Start splitting every chromosome simultaneously\n",
    "for chromosome in chromosomes:  # Make sure this is set for testing or production\n",
    "    if args.chr:\n",
    "        output_bam = file_name + '.' + chromosome + \".bam\"\n",
    "    else:\n",
    "        output_bam = file_name + '.chr' + chromosome + \".bam\"\n",
    "\n",
    "    command = \"samtools view -b %s %s > %s\" % (input_bam, chromosome, output_bam)\n",
    "    print(\"Command is: \" + command)\n",
    "    print(\"Output file is %s\" % output_bam)\n",
    "    p = subprocess.Popen(\"samtools view -b %s %s > %s\" % (input_bam, chromosome, output_bam), shell=True)\n",
    "    procs.append(p)\n",
    "    sys.stdout.flush()\n",
    "\n",
    "# Loop over all of the spawned jobs checking for completion\n",
    "while True:\n",
    "    counter = 0\n",
    "    for p in procs:\n",
    "        p.poll()\n",
    "        print (p.returncode)\n",
    "        sys.stdout.flush()\n",
    "        if p.returncode is not None:\n",
    "            counter += 1\n",
    "\n",
    "    # If all are completed break the loop to end the python script\n",
    "    if counter == len(chromosomes):  # Make sure this is set for testing or production\n",
    "        break\n",
    "\n",
    "    # Wait a little bit and check again\n",
    "    else:\n",
    "        print(\"Waiting 1 minute and check job status again\")\n",
    "        sys.stdout.flush()\n",
    "        time.sleep(60)\n",
    "        \n",
    "        ```\n",
    "```bash        \n",
    "#!/bin/bash\n",
    "\n",
    "#SBATCH --time=16:00:00 -n24 -p bynode\n",
    "\n",
    "#Args: Input_file.bam [--chr]\n",
    "# Use optional --chr if chromosome IDs start with 'chr'\n",
    "python3 split_bam_by_chromosome.py $1\n",
    "\n",
    "```\n",
    "## 2.4 merge bam files to each chrosome and create 24 files chr1-22,X,Y\n",
    "\n",
    "```bash\n",
    " #!/bin/sh\n",
    "\n",
    "#PBS -N merge_bam_by_chr\n",
    "#PBS -l nodes=1:ppn=4\n",
    "#PBS -l walltime=4:00:00\n",
    "#PBS -q dque\n",
    "\n",
    "cd $2\n",
    "\n",
    "# Get file name\n",
    "FILE=$(basename $1)\n",
    "\n",
    "# get the working directiory of the file\n",
    "INPUT_DIR=$(dirname $1)\n",
    "\n",
    "# create a sorted direction inside if it doesn't exist\n",
    "mkdir -p \"$INPUT_DIR\"/gathered\n",
    "\n",
    "# declare path for output file\n",
    "OUTPUT_FILE=\"$INPUT_DIR\"/gathered/\"${PWD##*/}\".\"$FILE\".gathered.bam\n",
    "\n",
    "METRICS=\"$INPUT_DIR\"/gathered/\"$FILE\".metrics.txt\n",
    "\n",
    "echo $FILE\n",
    "echo $OUT_FILE\n",
    "echo $METRICS\n",
    "\n",
    "bamtools merge -list $1 -out $OUTPUT_FILE\n",
    "#$1 is the file with contains list of files to merged to single chromosome\n",
    "\n",
    "```\n",
    "### At the end of this pipeline, we have 24 files for each sample, 720 files total."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3 Remove PCR bias- Deduplication"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 4 Bismark methylation extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5 Bismark important reports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
